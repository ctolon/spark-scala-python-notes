{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca128ef",
   "metadata": {},
   "source": [
    "### Spark Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e328650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ada9a",
   "metadata": {},
   "source": [
    "# Section 1: Create Spark Contexts with Different Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25924d77",
   "metadata": {},
   "source": [
    "## Create a Spark Context Method - 1: Using Only Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f94fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries we need\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11eb0ad",
   "metadata": {},
   "source": [
    "### Step 1: Create an Spark Session Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fdd8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"Create a RDD\") \\\n",
    ".config(\"spark.executor.memory\", \"4g\") \\\n",
    ".config(\"spark.driver.memory\", \"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a89747a",
   "metadata": {},
   "source": [
    "Params and Methods for Session Building\n",
    "\n",
    "SparkSession.builder -> Builder method\n",
    "master(\"local[4]\") -> How many cores used in local[<core>] (* for all, 4 for 4 core etc.)\n",
    "appName(\"Create a RDD\") -> application name\n",
    "config(\"spark.executor.memory\", \"4g\") -> Memory of Workers\n",
    "config(\"spark.driver.memory\", \"2g\") -> Our target host for send data and retrieving results\n",
    "getOrCreate() -> This method allows to if session created before, gets session else creates new session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdff28c",
   "metadata": {},
   "source": [
    "### Step 2: Create spark context object for create RDDs (for connection to computing cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40633e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730ece7",
   "metadata": {},
   "source": [
    "#### Create an RDD for using example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b37b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"Ahmet\", 25),(\"Berk\", 18),(\"Mehmet\", 28),(\"Batuhan\", 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0b176",
   "metadata": {},
   "source": [
    "#### Check RDD with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be4236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Berk', 18)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b532db2",
   "metadata": {},
   "source": [
    "#### We can stop spark session with stop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8f2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde9fb3",
   "metadata": {},
   "source": [
    "## Create a Spark Context Method - 2: Using Session and Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b78a4",
   "metadata": {},
   "source": [
    "### Step 1: Create an Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631fc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    ".setMaster(\"local[4]\") \\\n",
    ".setAppName(\"Create a RDD\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") \\\n",
    ".setExecutorEnv(\"spark.driver.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8553b18",
   "metadata": {},
   "source": [
    "### Step 2: Create an Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9085b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark = SparkSession.builder \\\n",
    ".config(conf=conf) \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c7ea5",
   "metadata": {},
   "source": [
    "### Step 3: Create spark context object for create RDDs (for connection to computing cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f089127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e017044",
   "metadata": {},
   "source": [
    "#### Create an RDD for using example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4fe7408",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"Ahmet\", 25),(\"Berk\", 18),(\"Mehmet\", 28),(\"Batuhan\", 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f6391",
   "metadata": {},
   "source": [
    "#### Check RDD with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456804bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Berk', 18)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5b58f",
   "metadata": {},
   "source": [
    "#### We can stop spark session with stop method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9d0193",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430691b3",
   "metadata": {},
   "source": [
    "## Create a Spark Context Method - 3: Using Context and Conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f5c18",
   "metadata": {},
   "source": [
    "### Step 1: Create an Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f660f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    ".setMaster(\"local[4]\") \\\n",
    ".setAppName(\"Create a RDD\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") \\\n",
    ".setExecutorEnv(\"spark.driver.memory\", \"2g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391ad36",
   "metadata": {},
   "source": [
    "### Step 2: Create spark context object for create RDDs (for connection to computing cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c2cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04a476",
   "metadata": {},
   "source": [
    "#### Create an RDD as tuple for using example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9b66c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([(\"Ahmet\", 25),(\"Berk\", 18),(\"Mehmet\", 28),(\"Batuhan\", 20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd238b",
   "metadata": {},
   "source": [
    "#### Check RDD with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfadd305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Berk', 18)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c05eae",
   "metadata": {},
   "source": [
    "#### Create an RDD as list for using example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36038f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize([[\"Ahmet\", 25],[\"Berk\", 18],[\"Mehmet\", 28],[\"Batuhan\", 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c87431",
   "metadata": {},
   "source": [
    "#### Check RDD with action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "628742e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ahmet', 25], ['Berk', 18], ['Mehmet', 28]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa08d0",
   "metadata": {},
   "source": [
    "#### Create an RDD as dict for using example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e74f0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_dict = {\"Sayilar\": [1,2,3,4,5], \"Harfler\": [\"a\",\"b\",\"c\",\"d\",\"e\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc760e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sayilar</th>\n",
       "      <th>Harfler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sayilar Harfler\n",
       "0        1       a\n",
       "1        2       b\n",
       "2        3       c\n",
       "3        4       d\n",
       "4        5       e"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = pd.DataFrame(my_dict)\n",
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d7b62bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o113.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:98)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:81)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rdd_from_pd_df \u001b[38;5;241m=\u001b[39m \u001b[43mpyspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m rdd_from_pd_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:406\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[1;32m    404\u001b[0m require_minimum_pandas_version()\n\u001b[0;32m--> 406\u001b[0m timezone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241m.\u001b[39msessionLocalTimeZone()\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/session.py:342\u001b[0m, in \u001b[0;36mSparkSession._jconf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_jconf\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;124;03m\"\"\"Accessor for the JVM SQL-specific configurations\"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msessionState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconf()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark-3.3.1-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o113.sessionState.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:98)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:81)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "rdd_from_pd_df = pyspark.createDataFrame(pd_df)\n",
    "rdd_from_pd_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45856b60",
   "metadata": {},
   "source": [
    "#### Create RDD From Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a7d1aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"OnlineRetail.csv\"\n",
    "rdd_text_file = sc.textFile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d145df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceNo;StockCode;Description;Quantity;InvoiceDate;UnitPrice;CustomerID;Country',\n",
       " '536365;85123A;WHITE HANGING HEART T-LIGHT HOLDER;6;1.12.2010 08:26;2,55;17850;United Kingdom',\n",
       " '536365;71053;WHITE METAL LANTERN;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;84406B;CREAM CUPID HEARTS COAT HANGER;8;1.12.2010 08:26;2,75;17850;United Kingdom',\n",
       " '536365;84029G;KNITTED UNION FLAG HOT WATER BOTTLE;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;84029E;RED WOOLLY HOTTIE WHITE HEART.;6;1.12.2010 08:26;3,39;17850;United Kingdom',\n",
       " '536365;22752;SET 7 BABUSHKA NESTING BOXES;2;1.12.2010 08:26;7,65;17850;United Kingdom',\n",
       " '536365;21730;GLASS STAR FROSTED T-LIGHT HOLDER;6;1.12.2010 08:26;4,25;17850;United Kingdom',\n",
       " '536366;22633;HAND WARMER UNION JACK;6;1.12.2010 08:28;1,85;17850;United Kingdom',\n",
       " '536366;22632;HAND WARMER RED POLKA DOT;6;1.12.2010 08:28;1,85;17850;United Kingdom']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_text_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a203d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
